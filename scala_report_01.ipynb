{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "//simple code Hello World\n",
    "println(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfatpclay: org.apache.spark.sql.DataFrame = [_c0: int, Surface: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "*Load csv file into dataframe using scala.\n",
    "*The options selected inlcude \n",
    "*inferschema set to True so that it automatically detects the data type of each column\n",
    "*delimiter set to comma and header set to True so that first line of the file becomes the header; \n",
    "*comparable command in pandas is --> data = pd.read_csv(file_name, encoding = 'ISO-8859-1')\n",
    "*/\n",
    "val dfatpclay = spark.read.options(Map(\"inferSchema\"->\"true\",\"delimiter\"->\",\",\"header\"->\"true\"))\n",
    "  .csv(\"/Users/psehgal/dev/airflow_home/Tennis_Data_Pipeline_Airflow_Project/images_for_reports/topclay.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Surface: string (nullable = true)\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Count_Win: integer (nullable = true)\n",
      " |-- Count_Lose: integer (nullable = true)\n",
      " |-- total_play: integer (nullable = true)\n",
      " |-- perc_win: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//print the schema of the dataframe\n",
    "dfatpclay.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfatpclay1: org.apache.spark.sql.DataFrame = [_c0: int, Surface: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Rename column name. It will create a new dataframe\n",
    "val dfatpclay1= dfatpclay.withColumnRenamed(\"total\", \"total_play\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Surface: string (nullable = true)\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Count_Win: integer (nullable = true)\n",
      " |-- Count_Lose: integer (nullable = true)\n",
      " |-- total_play: integer (nullable = true)\n",
      " |-- perc_win: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//print the schema of the new dataframe\n",
    "dfatpclay1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.sql.DataFrame = [_c0: int, Surface: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//If I change the column name of the existing dataframe\n",
    "dfatpclay.withColumnRenamed(\"total\", \"total_play\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Surface: string (nullable = true)\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Count_Win: integer (nullable = true)\n",
      " |-- Count_Lose: integer (nullable = true)\n",
      " |-- total_play: integer (nullable = true)\n",
      " |-- perc_win: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//and print the schema of the existing dataframe. you can see the column names did not change\n",
    "dfatpclay.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[396,Clay,Nadal R.,351,35,386,90.93]\n",
      "[135,Clay,Djokovic N.,169,39,208,81.25]\n",
      "[165,Clay,Federer R.,203,60,263,77.19]\n",
      "[304,Clay,Kuerten G.,105,40,145,72.41]\n",
      "[169,Clay,Ferrero J.C.,221,86,307,71.99]\n",
      "[405,Clay,Nishikori K.,60,24,84,71.43]\n",
      "[550,Clay,Thiem D.,59,24,83,71.08]\n",
      "[100,Clay,Coria G.,129,53,182,70.88]\n",
      "[168,Clay,Ferrer D.,293,122,415,70.6]\n",
      "[389,Clay,Moya C.,203,85,288,70.49]\n"
     ]
    }
   ],
   "source": [
    "// print the contents of dataframe\n",
    "//comparable command in pandas --> dfatpclay1\n",
    "dfatpclay1.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "//set libraryDependencies += \"org.apache.spark\" % \"spark_core\" % \"2.4.5\"\n",
    "//package org.apache.spark.sql\n",
    "//import com.redislabs.provider.redis._\n",
    "//import org.apache.spark.sql.redis\n",
    " \n",
    "\n",
    "//This is not working.. need to troubleshoot further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassNotFoundException",
     "evalue": " Failed to find data source: org.apache.spark.sql.redis. Please find packages at http://spark.apache.org/third-party-projects.html",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassNotFoundException: Failed to find data source: org.apache.spark.sql.redis. Please find packages at http://spark.apache.org/third-party-projects.html",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:245)",
      "  ... 36 elided",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.redis.DefaultSource",
      "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:418)",
      "  at java.lang.ClassLoader.loadClass(ClassLoader.java:351)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)",
      "  at scala.util.Try$.apply(Try.scala:192)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)",
      "  at scala.util.Try.orElse(Try.scala:84)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)",
      "  ... 37 more",
      ""
     ]
    }
   ],
   "source": [
    "//write the dataframe to redis database\n",
    "/*dfatpclay.write\n",
    "      .format(\"org.apache.spark.sql.redis\")\n",
    "      .option(\"table\", \"player\")\n",
    "      .save()\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
